{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created a Neural Network class. This class takes in list of Layer objects. It also takes a loss object.You can also allow it to take a seed as input. The seed is used for reproduciblity across runs. Each layer is characterized by its activation function and count of output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_activation_derivative(x):\n",
    "    return 1\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Clip x to prevent overflow.\n",
    "    x_clipped = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x_clipped))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    num_samples = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / num_samples\n",
    "    return loss\n",
    "\n",
    "def softmax_derivative(x):\n",
    "  return np.exp(x) / sum(np.exp(x)) * (1. - np.exp(x) / sum(np.exp(x)))\n",
    "\n",
    "\n",
    "def cross_entropy_loss_derivative(y_true, y_pred):\n",
    "    num_samples = y_true.shape[0]\n",
    "    return (y_pred - y_true) / num_samples\n",
    "\n",
    "def get_accuracy(Y_hat, Y):\n",
    "    # Convert one-hot encoded predictions to class labels\n",
    "    y_pred_labels = np.argmax(Y_hat, axis=1)\n",
    "    # Convert one-hot encoded true labels to class labels\n",
    "    y_true_labels = np.argmax(Y, axis=1)\n",
    "    # Calculate accuracy by comparing predicted labels with true labels\n",
    "    accuracy = (y_pred_labels == y_true_labels).mean()*100\n",
    "    return accuracy\n",
    "\n",
    "def get_predictions(AL):\n",
    "    # Step 1: Find the index of the max probability in each row\n",
    "    max_indices = np.argmax(AL, axis=1)\n",
    "\n",
    "    # Step 2: Create a new matrix of zeros with the shape (number of rows in AL, number of classes)\n",
    "    # Assuming number of classes is equal to the number of rows in AL for a single sample scenario\n",
    "    one_hot_predictions = np.zeros_like(AL)\n",
    "\n",
    "    # Step 3: Set the elements to 1 at each row's max index\n",
    "    # np.arange(AL.shape[0]) creates an array of row indices, max_indices specifies the column for each row\n",
    "    one_hot_predictions[np.arange(AL.shape[0]), max_indices] = 1\n",
    "\n",
    "    return one_hot_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1  # Corrected dimensions\n",
    "        self.biases = np.random.rand() * 10\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.output = None\n",
    "        self.input = None\n",
    "        self.activation_input = None\n",
    "        self.d_weights = None\n",
    "        self.d_biases = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        # Correctly apply dot product given the input and weight dimensions\n",
    "        self.activation_input = np.dot(input_data, self.weights) + self.biases\n",
    "        self.output = self.activation_function(self.activation_input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        d_activation_input = output_error * self.activation_derivative(self.activation_input)\n",
    "\n",
    "        # Correct gradient calculation based on the updated dot product order\n",
    "        self.d_weights = np.dot(self.input.T, d_activation_input)\n",
    "        self.d_biases = np.sum(d_activation_input, axis=0, keepdims=True)  # Corrected axis for biases\n",
    "\n",
    "        # Correct backpropagated error calculation\n",
    "        input_error = np.dot(d_activation_input, self.weights.T)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * self.d_weights\n",
    "        self.biases -= learning_rate * self.d_biases\n",
    "\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss, loss_derivative, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.loss_value=None\n",
    "        self.loss_derivative = loss_derivative\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data\n",
    "\n",
    "    def backward(self, y_true, learning_rate):\n",
    "            \"\"\"\n",
    "            Perform backward propagation through the neural network.\n",
    "            \"\"\"\n",
    "            output_error = self.loss_derivative(y_true, self.layers[-1].output)\n",
    "            input_error = output_error\n",
    "            for layer in reversed(self.layers):\n",
    "                input_error = layer.backward(input_error, learning_rate)\n",
    "            return input_error\n",
    "    def train_sgd(self, x_train, y_train, learning_rate, epochs, batch_size):\n",
    "        n_samples = x_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the dataset at the beginning of each epoch\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            x_train_shuffled = x_train[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "            \n",
    "            # Stochastic Gradient iteration\n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                x_batch = x_train_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward and backward passes for the Stochastic \n",
    "                self.forward(x_batch)\n",
    "                self.backward(y_batch, learning_rate)\n",
    "            \n",
    "            \n",
    "            output = self.forward(x_train)\n",
    "            epoch_loss = self.loss(y_train, output)\n",
    "            # print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss}\")\n",
    "    \n",
    "    # def train(self, x_train, y_train, learning_rate):\n",
    "\n",
    "    # # Assuming x_train is a 2D array where each row is a sample\n",
    "    # # and y_train is a 2D array where each row is a target value or vector\n",
    "    #     loss=0;\n",
    "\n",
    "    #     # Reshape the i-th sample to a column vector\n",
    "    #     # x_sample = x_train\n",
    "    #     # y_sample = y_train\n",
    "\n",
    "    #     # Forward propagation\n",
    "    #     output = self.forward(x_train)\n",
    "\n",
    "    #     # Compute loss\n",
    "    #     loss = self.loss(y_train, output)\n",
    "    #     self.loss_value=loss\n",
    "\n",
    "    #     # Backward propagation\n",
    "    #     self.backward(y_train, learning_rate)\n",
    "\n",
    "\n",
    "    #     # print(f\"Iteration: Loss: {loss}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Just one output neural with linear activation and least mean square loss.(This is linear regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Fetch the California Housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "Y = california_housing.target.reshape(-1, 1)  # Ensure Y is a column vector\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "# Assuming your classes and functions are defined as in the provided code snippet\n",
    "\n",
    "# Initialize layers\n",
    "input_size = X_train.shape[1]  # Number of features in the dataset\n",
    "layer1 = Layer(input_size=input_size, output_size=1 ,activation_function=linear, activation_derivative=linear_activation_derivative)\n",
    "\n",
    "# Initialize the neural network\n",
    "nn1 = NeuralNetwork(layers=[layer1], loss=mse_loss, loss_derivative=mse_loss_derivative, seed=42)\n",
    "# Training settings\n",
    "epochs = 200 # Number of epochs to train\n",
    "learning_rate = 0.000001# Learning rate\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "        # Train the neural network on the sample\n",
    "    nn1.train_sgd(X_train, Y_train, learning_rate,50,100)\n",
    "    if epoch%1==0:\n",
    "      output1 = nn1.forward(X_train)\n",
    "      loss1 = mse_loss(Y_train,output1)\n",
    "      losses.append(loss1)\n",
    "      print(f\"Epoch {epoch+1}/{epochs} {loss1}\")\n",
    "      \n",
    "\n",
    "output1 = nn1.forward(X_train)\n",
    "loss1 = mse_loss(Y_train,output1)\n",
    "print(\"loss1 train :\",loss1)\n",
    "output1 = nn1.forward(X_test)\n",
    "loss1 = mse_loss(Y_test,output1)\n",
    "print(\"loss1 test :\",loss1)\n",
    "\n",
    "plt.plot(losses,color=\"green\")\n",
    "plt.xlabel(\"Iteration count\")\n",
    "plt.ylabel(\"Loss of Neural Network 2.1\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Two layers. Layer 1 with 13 output neurons with sigmoid activation. Layer 2 with one output neuron and linear activation. use mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Layer(input_size=input_size, output_size=13 ,activation_function=sigmoid, activation_derivative=sigmoid_derivative)\n",
    "layer2 = Layer(input_size=13, output_size=1 ,activation_function=linear, activation_derivative=linear_activation_derivative)\n",
    "nn2 = NeuralNetwork(layers=[layer1,layer2], loss=mse_loss, loss_derivative=mse_loss_derivative, seed=42)\n",
    "epochs = 200  # Number of epochs to train\n",
    "learning_rate = 0.001 # Learning rate\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "        # Train the neural network on the sample\n",
    "    nn2.train_sgd(X_train, Y_train, learning_rate,50,100)\n",
    "    if epoch%1==0:\n",
    "        output2 = nn2.forward(X_train)\n",
    "        loss2 = mse_loss(Y_train,output2)\n",
    "        losses.append(loss2)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} {loss2}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output2 = nn2.forward(X_train)\n",
    "loss2 = mse_loss(Y_train,output2)\n",
    "print(\"loss2 train :\",loss2)\n",
    "output2 = nn2.forward(X_test)\n",
    "loss2 = mse_loss(Y_test,output2)\n",
    "print(\"loss1 test:\",loss2)\n",
    "\n",
    "plt.plot(losses,color=\"green\")\n",
    "plt.xlabel(\"Iteration count\")\n",
    "plt.ylabel(\"Loss of Neural Network 2.2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Three layers. Layer 1 with 13 output neurons with sigmoid activation.Layer 2 with 13 output neurons and sigmoid activation. Layer 3 with one output neuron and linear activation. use mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Layer(input_size=input_size, output_size=13 ,activation_function=sigmoid, activation_derivative=sigmoid_derivative)\n",
    "layer2 = Layer(input_size=13, output_size=13 ,activation_function=sigmoid, activation_derivative=sigmoid_derivative)\n",
    "layer3 = Layer(input_size=13, output_size=1 ,activation_function=linear, activation_derivative=linear_activation_derivative)\n",
    "nn3 = NeuralNetwork(layers=[layer1,layer2,layer3], loss=mse_loss, loss_derivative=mse_loss_derivative, seed=42)\n",
    "epochs = 100  # Number of epochs to train\n",
    "learning_rate = 0.01# Learning rate\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "        # Train the neural network on the sample\n",
    "    nn3.train_sgd(X_train, Y_train, learning_rate,50,100)\n",
    "    if epoch%1==0:\n",
    "        output3 = nn3.forward(X_train)\n",
    "        loss3 = mse_loss(Y_train,output3)\n",
    "        losses.append(loss3)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} {loss3}\")\n",
    "output3 = nn3.forward(X_train)\n",
    "loss3 = mse_loss(Y_train,output3)\n",
    "print(\"loss3 train:\",loss3)\n",
    "output3 = nn3.forward(X_test)\n",
    "loss3 = mse_loss(Y_test,output3)\n",
    "print(\"loss3 test:\",loss3)\n",
    "\n",
    "plt.plot(losses,color=\"green\")\n",
    "plt.xlabel(\"Iteration count\")\n",
    "plt.ylabel(\"Loss of Neural Network 2.3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Two layers Layer 1 with 89 output neurons with tanh activation. Layer 2 with ten output neuron and sigmoid activation. use mean squared loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_openml\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Fetch the MNIST dataset\n",
    "# mnist = fetch_openml('mnist_784', version=1,as_frame=False)\n",
    "# X = mnist.data\n",
    "# Y = mnist.target.astype('int32')  # Convert target to integer\n",
    "\n",
    "# # Normalize features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # One-hot encode the labels\n",
    "# encoder = OneHotEncoder(categories='auto')\n",
    "# Y_one_hot = encoder.fit_transform(Y.reshape(-1, 1)).toarray()\n",
    "\n",
    "# # Split the dataset into training and test sets\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "# Extract features (X) and target labels (Y)\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "shuffle_indices = np.random.permutation(len(X))\n",
    "X,Y= X[shuffle_indices], Y[shuffle_indices]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the labels (not necessary for this classification task)\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "Y_one_hot = encoder.fit_transform(Y.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_one_hot, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "# print(X[0])\n",
    "# print(Y_one_hot)\n",
    "input_size = X_train.shape[1]  # Number of features in the dataset\n",
    "layer1 = Layer(input_size=input_size, output_size=89 ,activation_function=tanh, activation_derivative=tanh_derivative)\n",
    "layer2 = Layer(input_size=89, output_size=10 ,activation_function=sigmoid, activation_derivative=sigmoid_derivative)\n",
    "\n",
    "# Initialize the neural network\n",
    "nn_digits = NeuralNetwork(layers=[layer1, layer2], loss=mse_loss, loss_derivative=mse_loss_derivative, seed=42)\n",
    "# Training settings\n",
    "epochs = 100 # Number of epochs to train\n",
    "learning_rate = 0.5# Learning rate\n",
    "accuracies = []\n",
    "for epoch in range(epochs):\n",
    "    nn_digits.train_sgd(X_train, Y_train, learning_rate,50,100)\n",
    "    y_hat1 = nn_digits.forward(X_train)\n",
    "    # print(y_hat)\n",
    "    y_hat = get_predictions(y_hat1)\n",
    "    # print(y_hat)\n",
    "    if epoch%1==0:\n",
    "      accuracies.append(get_accuracy(y_hat,Y_train))\n",
    "      print(f\"Epoch {epoch+1}/{epochs} {get_accuracy(y_hat,Y_train)}\")\n",
    "\n",
    "output_digits_train = nn_digits.forward(X_train)\n",
    "y_hat_digits_train = get_predictions(output_digits_train)\n",
    "accuracy = get_accuracy(y_hat_digits_train,Y_train)\n",
    "print(\"Accuracy digits train :\",accuracy)\n",
    "\n",
    "output_digit_test = nn_digits.forward(X_test)\n",
    "y_hat_digit_test = get_predictions(output_digit_test)\n",
    "accuracy = get_accuracy(y_hat_digit_test,Y_test)\n",
    "print(\"Accuracy digits test :\",accuracy)\n",
    "\n",
    "plt.plot(accuracies,color=\"green\")\n",
    "plt.xlabel(\"Iteration count\")\n",
    "plt.ylabel(\"Accuracy of Neural Network 3.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Two layers. Layer 1 with 89 output neurons with tanh activation. Layer 2 with ten output neuron and linear activation. use softmax with cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]  # Number of features in the dataset\n",
    "layer1 = Layer(input_size=input_size, output_size=89 ,activation_function=tanh, activation_derivative=tanh_derivative)\n",
    "layer2 = Layer(input_size=89, output_size=10 ,activation_function=softmax, activation_derivative=softmax_derivative)\n",
    "nn_digit2 = NeuralNetwork(layers=[layer1, layer2], loss=cross_entropy_loss, loss_derivative=cross_entropy_loss_derivative, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training settings\n",
    "epochs = 60  # Number of epochs to train\n",
    "learning_rate = 0.005 # Learning rate\n",
    "accuracies = []\n",
    "for epoch in range(epochs):\n",
    "    nn_digit2.train_sgd(X_train, Y_train, learning_rate,50,100)\n",
    "    y_hat1 = nn_digit2.forward(X_train)\n",
    "    # print(y_hat)\n",
    "    y_hat = get_predictions(y_hat1)\n",
    "    # print(y_hat)\n",
    "    if epoch%1==0:\n",
    "      accuracies.append(get_accuracy(y_hat,Y_train))\n",
    "      print(f\"Epoch {epoch+1}/{epochs} {get_accuracy(y_hat,Y_train)}\")\n",
    "\n",
    "\n",
    "output_digits_train = nn_digit2.forward(X_train)\n",
    "y_hat_digits_train = get_predictions(output_digits_train)\n",
    "accuracy = get_accuracy(y_hat_digits_train,Y_train)\n",
    "print(\"Accuracy digit train :\",accuracy)\n",
    "\n",
    "output_digit_test = nn_digit2.forward(X_test)\n",
    "y_hat_digit_test = get_predictions(output_digit_test)\n",
    "accuracy = get_accuracy(y_hat_digit_test,Y_test)\n",
    "print(\"Accuracy digit test :\",accuracy)\n",
    "\n",
    "plt.plot(accuracies,color=\"green\")\n",
    "plt.xlabel(\"Iteration count\")\n",
    "plt.ylabel(\"Accuracy of Neural Network 3.2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(1 / (input_size + output_size))  # Corrected dimensions\n",
    "        self.biases = 0\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.output = None\n",
    "        self.input = None\n",
    "        self.activation_input = None\n",
    "        self.d_weights = None\n",
    "        self.d_biases = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        # Correctly apply dot product given the input and weight dimensions\n",
    "        self.activation_input = np.dot(self.weights.T,input_data) + self.biases\n",
    "        self.output = self.activation_function(self.activation_input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        # print(output_error.shape)\n",
    "        # print(self.activation_derivative(self.activation_input).shape)\n",
    "        d_activation_input = output_error * self.activation_derivative(self.activation_input)\n",
    "        #print(d_activation_input.shape)\n",
    "        # Correct gradient calculation based on the updated dot product order\n",
    "        self.d_weights = np.dot(self.input, d_activation_input.T)\n",
    "        self.d_biases = np.sum(d_activation_input, axis=0, keepdims=True)  # Corrected axis for biases\n",
    "\n",
    "        # Correct backpropagated error calculation\n",
    "        input_error = np.dot(self.weights,d_activation_input)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * self.d_weights\n",
    "        self.biases -= learning_rate * self.d_biases\n",
    "\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss, loss_derivative, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.loss_value=None\n",
    "        self.loss_derivative = loss_derivative\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data\n",
    "\n",
    "    def backward(self, y_true, learning_rate):\n",
    "            \"\"\"\n",
    "            Perform backward propagation through the neural network.\n",
    "\n",
    "            Args:\n",
    "            y_true (numpy.ndarray): True labels.\n",
    "            learning_rate (float): Learning rate for updating weights and biases.\n",
    "\n",
    "            Returns:\n",
    "            numpy.ndarray: Gradient of loss with respect to the input of the first layer.\n",
    "            \"\"\"\n",
    "            output_error = self.loss_derivative(y_true.T, self.layers[-1].output)\n",
    "\n",
    "            # print(y_true.shape)\n",
    "            # print(self.layers[-1].output.shape)\n",
    "            # print(\"out:\",output_error.shape)\n",
    "            input_error = output_error\n",
    "            for layer in reversed(self.layers):\n",
    "                input_error = layer.backward(input_error, learning_rate)\n",
    "            return input_error\n",
    "    def train(self, x_train, y_train, learning_rate):\n",
    "\n",
    "    # Assuming x_train is a 2D array where each row is a sample\n",
    "    # and y_train is a 2D array where each row is a target value or vector\n",
    "        loss=0\n",
    "\n",
    "        # Reshape the i-th sample to a column vector\n",
    "        # x_sample = x_train\n",
    "        # y_sample = y_train\n",
    "\n",
    "        # Forward propagation\n",
    "        output = self.forward(x_train)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss(y_train, output)\n",
    "        self.loss_value=loss\n",
    "\n",
    "        # Backward propagation\n",
    "        self.backward(y_train, learning_rate)\n",
    "\n",
    "\n",
    "        # print(f\"Iteration: Loss: {loss}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv_Layer:\n",
    "    def __init__(self, input_shape, output_channels, kernel_size, stride, padding,alpha):\n",
    "        self.inp_shape = input_shape\n",
    "        self.input_channel = input_shape[0]\n",
    "        self.inp_height = input_shape[1]\n",
    "        self.inp_width = input_shape[2]\n",
    "\n",
    "        self.output_channels = output_channels\n",
    "        self.output_shape = (output_channels,self.inp_height,self.inp_width)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = 1\n",
    "        self.inp = np.random.rand(*input_shape)\n",
    "        self.out = np.random.rand(*self.output_shape)\n",
    "        self.kernel_shape = (3,3)\n",
    "        #self.inp_shape =\n",
    "        # Initialize weights and biases\n",
    "        self.filters_shape = (self.output_channels,self.input_channel, *self.kernel_shape)\n",
    "        self.filters = np.random.random((self.output_channels,self.input_channel, kernel_size, kernel_size))\n",
    "        self.biases = np.random.rand(*self.output_shape)\n",
    "        self.alpha = alpha\n",
    "        # # Cache variables for backward pass\n",
    "        # self.cache = None\n",
    "\n",
    "\n",
    "    def conv(self,inp,weights):\n",
    "      #print(inp.shape[0])\n",
    "      m = inp.shape[0]+2\n",
    "      zero_pad_inp = np.zeros((m,m))\n",
    "      zero_pad_inp[1:inp.shape[0]+1,1:inp.shape[1]+1] = inp\n",
    "\n",
    "      l = zero_pad_inp.shape[0] - weights.shape[0]+1\n",
    "      w = zero_pad_inp.shape[1] - weights.shape[1]+1\n",
    "      output = np.zeros((l,w))\n",
    "\n",
    "      for i in range(l):\n",
    "        for j in range(w):\n",
    "          mat = zero_pad_inp[i:i+weights.shape[0],j:j+weights.shape[1]]\n",
    "          mat_n = np.multiply(mat,weights)\n",
    "          output[i,j] = np.sum(mat_n)\n",
    "\n",
    "      return output\n",
    "\n",
    "\n",
    "    def forward(self,):\n",
    "       #print(\"con_frwrd\")\n",
    "       self.out = np.zeros(self.output_shape)\n",
    "       for i in range(self.output_channels):\n",
    "            for j in range(self.input_channel):\n",
    "                self.out[i]+= self.conv(self.inp[j],self.filters[i,j])\n",
    "\n",
    "       flatten_out = self.out.reshape(-1,1)\n",
    "       return flatten_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Convolution operation\n",
    "\n",
    "\n",
    "    def backward(self,dl_do):\n",
    "      dl_do = np.reshape(dl_do,self.output_shape)\n",
    "      self.dl_df = np.zeros(self.filters_shape)\n",
    "      self.dl_db = np.zeros_like(self.output_shape)\n",
    "\n",
    "      for i in range(self.output_channels):\n",
    "        for j in range(self.input_channel):\n",
    "          self.dl_df[i,j] = self.conv(self.inp[j],dl_do[i])\n",
    "\n",
    "      self.filters-=self.alpha*self.dl_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "  def __init__(self,conv_layer,neural_network,X,Y,alpha):\n",
    "    self.conv_layer = conv_layer\n",
    "    self.neural_network = neural_network\n",
    "    self.X=X\n",
    "    self.Y =Y\n",
    "    self.loss=_\n",
    "    self.y_pred=_\n",
    "    self.alpha=alpha\n",
    "  def forward_prop(self,):\n",
    "    #print(\"frwrd\")\n",
    "    self.conv_layer.inp = self.X\n",
    "    self.Z = self.conv_layer.forward()\n",
    "    X = tanh(self.Z)\n",
    "    self.y_pred = self.neural_network.forward(X)\n",
    "    #print(self.y_pred)\n",
    "    self.loss = cross_entropy_loss(self.Y,self.y_pred)\n",
    "\n",
    "  def backward_pass(self,):\n",
    "    dl_dtanhx = self.neural_network.backward(self.Y,self.alpha)\n",
    "    dtanhx_dx = tanh_derivative(self.Z)\n",
    "    dl_dx = np.dot(dtanhx_dx,dl_dtanhx.T)\n",
    "    self.conv_layer.backward(dl_dx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalize this for any number of input and any number of output channel. Implement both forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sg_cnn(X_train,Y_train,X_test,Y_test,cnn,iterations,inp_shape,out_shape):\n",
    "  for i in range(iterations):\n",
    "    for j in range(X_train.shape[0]):\n",
    "      X_sample = X_train[j].reshape(inp_shape)\n",
    "      Y_sample = Y_train[j].reshape(out_shape)\n",
    "      cnn.X = X_sample\n",
    "      cnn.Y = Y_sample\n",
    "      cnn.forward_prop()\n",
    "      cnn.backward_pass()\n",
    "    print(\"Loss at\",i,cnn.loss)\n",
    "\n",
    "  col_num = X_train.shape[0]\n",
    "  X_train = X_train.reshape(col_num,8,8)\n",
    "  acc=0\n",
    "  for i in range(len(X_test)):\n",
    "    X_new = X_test[i].reshape(inp_shape)\n",
    "    cnn.X = X_new\n",
    "    cnn.Y = Y_test[i].reshape(out_shape)\n",
    "    cnn.forward_prop()\n",
    "    y_pred = np.argmax(cnn.y_pred)\n",
    "    y_true = np.argmax(Y_test[i])\n",
    "    if(y_pred==y_true):\n",
    "      acc+=1\n",
    "  print(acc)\n",
    "  print(\"Acc : \",(acc/len(Y_test))*100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "digits['data'] = 1*(digits['data'] >= 8)\n",
    "X = digits['data']\n",
    "Y = digits['target'].reshape(-1,1)\n",
    "\n",
    "encode = OneHotEncoder()\n",
    "Y = encode.fit_transform(Y).toarray()\n",
    "\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained this CNN on mnist dataset. Layer 1: Convolution layer with 16 output channels+flatten +tanh activation. Layer 2: 10 output neuron with linear activation. Softmax cross entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (self, input_shape, output_channels, kernel_size, stride, padding,alpha)\n",
    "output_channels = 16\n",
    "input_shape = (1,8,8)\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "learning_rate = 0.01\n",
    "conv_layer_obj = Conv_Layer(input_shape,output_channels,kernel_size,stride,padding,learning_rate)\n",
    "\n",
    "# input_size, output_size, activation_function, activation_derivative\n",
    "# Layer(input_size=13, output_size=1 ,activation_function=linear, activation_derivative=linear_activation_derivative)\n",
    "input_size = output_channels*8*8\n",
    "output_size = 10\n",
    "layer1_obj = Layer(input_size,output_size,activation_function=softmax,activation_derivative = softmax_derivative)\n",
    "layers=[layer1_obj]\n",
    "# layers, loss, loss_derivative, seed=None\n",
    "neur_net_obj = NeuralNetwork(layers,cross_entropy_loss,cross_entropy_loss_derivative,42)\n",
    "cnn_obj = CNN(conv_layer_obj,neur_net_obj,X_train,Y_train,0.001)\n",
    "out_shape = (1,10)\n",
    "iterations = 100\n",
    "#print(X_train.shape)\n",
    "sg_cnn(X_train,Y_train,X_test,Y_test,cnn_obj,iterations,input_shape,out_shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
